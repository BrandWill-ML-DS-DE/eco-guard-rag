import streamlit as st
import os
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_community.llms import Groq


st.set_page_config(page_title="Eco-Guard Hybrid RAG", layout="wide")

with st.sidebar:
    st.title("üõ†Ô∏è Settings")
    google_api_key = st.text_input("Gemini API Key", type="password")
    groq_api_key = st.text_input("Groq API Key (for SLM)", type="password")
    
    routing_mode = st.toggle("Enable Smart Routing", value=True, 
                             help="Uses SLM for summaries, LLM for complex logic.")


def process_document(uploaded_file):
    with open("temp.pdf", "wb") as f:
        f.write(uploaded_file.getbuffer())
    
    loader = PyPDFLoader("temp.pdf")
    data = loader.load()
    
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    chunks = splitter.split_documents(data)
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001", google_api_key=google_api_key)
    vector_db = Chroma.from_documents(chunks, embeddings)
    return vector_db


def get_response(query, vector_db):
    # Intent Detection (The "Senior" part)
    is_complex = any(word in query.lower() for word in ["analyze", "risk", "conflict", "legal", "why"])
    
    if not routing_mode or is_complex:
        # High-Reasoning Path (LLM)
        model_name = "Gemini 1.5 Flash"
        llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", google_api_key=google_api_key)
    else:
        # Cost-Effective Path (SLM via Groq)
        model_name = "Phi-3 Mini (SLM)"
        from langchain_community.chat_models import ChatGroq
        llm = ChatGroq(temperature=0, groq_api_key=groq_api_key, model_name="phi3-7b-8192")

    qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=vector_db.as_retriever())
    return qa_chain.run(query), model_name

st.header("üìÑ Eco-Guard: Hybrid Compliance Analyzer")
uploaded_file = st.file_uploader("Upload a Contract or Technical Doc", type="pdf")

if uploaded_file and google_api_key:
    if "vector_db" not in st.session_state:
        with st.spinner("Indexing document..."):
            st.session_state.vector_db = process_document(uploaded_file)
            st.success("Document Indexed!")

    query = st.text_input("Ask a question about the document:")
    if query:
        response, model_used = get_response(query, st.session_state.vector_db)
        
        st.info(f"Generated by: **{model_used}**")
        st.markdown(f"### Answer:\n{response}")
